\section{RELATED WORK} \label{relatedwork}

Since their appearance in literature, GANs have been successfully applied to problems of image generation, editing and semi-supervised learning \cite{DBLP:journals/corr/RadfordMC15} \cite{DBLP:journals/corr/ZhangXLZHWM16}. \textit{The results obtained with this technique were so good that they captured the attention of a great number of researchers, leading to a proliferation of various flavors of GAN, each performing better than the others on a specific domain.} It's difficult anyway to understand how to compare different GAN models, because of the lack of a consistent metric and the different architectures with which networks can be designed, which for each project are related to the corresponding computational budget. A tentative to define some guidelines to avoid these problems, together with a fair and comprehensive comparison of state-of-the-art GANs, is discussed in \cite{46506}[MISSING ACCENTS ON REF]: what emerges here is that the computational budget plays a major role, allowing bad algorithms to outperform good ones if given enough time; plus, despite many claims of algorithms being superior to the original ones, there is no empirical evidence that they are across all datasets, in fact the original model outperforms them in most of them. [INSERT TABLE WITH LOSSES].
We thus report here a formalization of the original problem \cite{NIPS2014_5423}, modeling it with a game theory approach in a more precise way, as done in \cite{2017arXiv171200679O}.

Deep Convolutional GAN (DCGAN) can be naturally represented as a 2-player zero-sum infinite game: the player entities, G and D, are two neural networks, each with a given architecture defining its depth and width; pure strategies are the different combinations of weights for the given architecture, and it is immediate to notice that these numbers are infinite; payoffs are defined as the opposite of their loss functions, which are also used for the training phase of the networks.
An infinite game doesn't guarantee the existence of a NE, not even in mixed strategies, nor it allows to find it as the value of the game, i.e. maximinimizing the payoffs. As pointed out in  \cite{2017arXiv171200679O} though, when using floating point numbers, the number of combinations of strategies becomes finite, albeit very large: there is then a NE, at least in mixed strategies, which however is hard to compute. It has to be kept into account also the fact that the optimization of a neural network can lead to a Local NE (LNE) because the problem is non-convex.

In strategic form, GAN games can be described by a tuple $(\{G, D\}, \{S_G, S_D\}, \{u_G, u_D\})$, where $S_G$ and $S_D$ are the (finite) sets of strategies while $u_G$ and $u_D$ are the corresponding payoffs, for G and D respectively.
The loss functions used for DCGAN training are: [CORREGGERE]
\begin{align*}
	L_D = E_{x \sim p_d}[\log(D(x))]+E_{x \sim p_g}[\log(1-D(x))]\\
	L_G = E_{x \sim p_d}[\log(D(x))]+E_{x \sim p_g}[\log(1-D(x))]
\end{align*}
which correspond to the binary cross-entropy loss functions. Each of the two players should then minimize its own loss function, which is equivalent to maximizing its payoff. The problem is then reduced to a maximin problem, where the value of the game is defined as 