\section{RESULTS} \label{results}

We first report here the results obtained on the two 2D datasets. In FIG.X the losses for both D and G are plotted, as functions of the epochs. It can be immediately noticed that for both data distributions, the highest loss for the discriminator is obtained with $\alpha=0.5$ which foretells the training of a good generator (notice that for this $\alpha$, as expected, $L^{(D)}$ converges to $\ln(2)=0.693$). Anyway also with $\alpha=\{0.3,0.7\}$ good results are achieved in this sense. From the generator's losses it can be noticed that, the lower $\alpha$, the faster the loss convergence to the final value. An insight on how well are points generated is given in FIG.X, where for different values of $\alpha$ we can see how points are generated after 30000 training epochs: these results confirm what expected, that is better performances from more balanced number of true images per mini-batch.

When applying the model to the MNIST dataset, similar results are obtained FIG.X, with losses for the discriminator approaching this time $0.5$ for $\alpha=\{0.3,0.5,0.7\}$. Also in this data space, as pointed out in the figure, when mini-batches are more balanced the D loss is higher, indicating better generators.
The evolution of how numbers are generated is depicted in FIG.X, where for each $\alpha$, every 20 and up to 100 epochs, a grid with 36 outputs are displayed. Also here for lower values of $\alpha$, the generator outputs acceptable images much faster, as observed for the 2D case.