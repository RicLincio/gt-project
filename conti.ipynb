{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "#### Zero-Sum Game Losses\n",
    "* Disciminator Loss : $L^{(D)} = -\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] - \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]$\n",
    "* Generator Loss : $L^{(G)} = - L^{(D)}$\n",
    "\n",
    "#### Heuristic Game Losses\n",
    "In order to speed up the training phase Goodfellow approach is to change the loss of the generator:\n",
    "* Disciminator Loss : $L^{(D)} = -\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] - \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))]$\n",
    "* Generator Loss : $L^{(G)} = -\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (D(G(z)))]$\n",
    "\n",
    "#### Maximum-Likelihood Game Losses\n",
    "* Disciminator Loss : $L^{(D)}$ = \n",
    "* Generator Loss : $L^{(G)}$ = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Formulation (for the zero-sum game)\n",
    "$$\\min\\limits_{G} \\max\\limits_{D} \\big\\{V(D,G)\\big\\} = \\min\\limits_{G} \\max\\limits_{D} \\Big\\{\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] \\Big\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition 1\n",
    "For fixed $\\bar{G}$ the optimal discriminator $D$ is :\n",
    "\n",
    "$$D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n",
    "\n",
    "#### Proof\n",
    "We Consider the following optimization problem:\n",
    "$$D^*(x) = arg\\max\\limits_{D} \\big\\{V(D,G) \\big\\}$$\n",
    "\n",
    "$$\\Rightarrow V(G,D) = \\int_x p_{data(x)} \\log(D(x)) dx + \\int_z p_{z}(z) \\log(1-D(G(z))) dz = $$\n",
    "$$= \\int_x \\Big[p_{data(x)} \\log(D(x)) + p_{g}(x) \\log(1-D(x)) \\Big]dx = $$\n",
    "$$= \\int_x L\\big(x,D(x),\\dot{D}(x)\\big)dx$$\n",
    "\n",
    "If $D(x)$ is an optimal point it satisfy the Euler-Lagrange equation:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial D} = \\frac{d L}{d x} \\frac{\\partial L}{\\partial \\dot{D}}$$\n",
    "Since $ \\partial L / \\partial \\dot{D} = 0$\n",
    "$$\\frac{\\partial L}{\\partial D} = \\frac{p_{data}(x)}{D(x)} - \\frac{p_g(x)}{1-D(x)} = 0 \\ \\ \\Rightarrow \\ \\ D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_g = p_{data}$. At that point, $C(G)$ achievesthevalue $âˆ’\\log 4$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrization of the Losses\n",
    "$$\\min\\limits_{G} \\max\\limits_{D} \\big\\{V_{\\alpha}(D,G)\\big\\} = \\min\\limits_{G} \\max\\limits_{D} \\Big\\{\\alpha\\cdot\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + (1-\\alpha)\\cdot\\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1-D(G(z)))] \\Big\\}$$\n",
    "\n",
    "$$\\Rightarrow \\ \\ D_{\\alpha}^*(x) = \\frac{\\alpha \\cdot p_{data}(x)}{\\alpha \\cdot p_{data}(x) + (1-\\alpha)\\cdot p_g(x)}$$\n",
    "\n",
    "$$\\Rightarrow \\ \\ p_g(x) = p_{data}(x)$$\n",
    "\n",
    "$$\\Rightarrow \\ \\ D_{\\alpha}^*(x) = \\alpha$$\n",
    "\n",
    "$$L^{(D)} \\rightarrow -\\log(\\alpha) -\\log(1-\\alpha)$$\n",
    "$$L^{(G)} \\rightarrow -\\log(\\alpha)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
