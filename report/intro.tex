
\section{Introduction}
\label{sec:introduction}

GANs, namely Generative Adversarial Networks, are a hot topic nowadays. Since their first description in \cite{} they have gained more and more momentum because they are able to sidestep some of the difficulties encountered when trying to apply deep learning to generative models: until then, deep learning has had indeed a greater impact on discriminative models, leaving less success to deep generative ones. The purpose of a generative model is, given a training set of sample data distributed according to an unknown probability density function (pdf) $p_{data}$, to generate samples with a distribution $p_g$ that mimics $p_{data}$. (This can be done in several ways, e.g. implicitly or explicitly, â€¦)
[HOW] As suggested by the name, in GAN framework this is achieved by putting in competition two entities: a generator and a discriminator. The task of the generator (G) is to generate data that can be regarded as true by the discriminator, while the discriminator (D) has the purpose of correctly classifying real data and fake data. The classical real-life analogy with this process involves counterfeiters trying to produce fake currency and the police trying to detect it. Usually these two entities are implemented with neural networks, so that the process by which they learn is of trial and error. However the GANs are difficult to train, because they present several implementation problems which lead to results that might be different from what expected.
[GENERATIVE MODELS]
[] Ideally then $p_g = p_{data}$, but since the latter is unknown the objective of the training should be defined in a slightly different way. 
