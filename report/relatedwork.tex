\section{RELATED WORK} \label{relatedwork}

Since their appearance in literature, GANs have been successfully applied to problems of image generation, editing and semi-supervised learning \cite{DBLP:journals/corr/RadfordMC15} \cite{DBLP:journals/corr/ZhangXLZHWM16}. \textit{The results obtained with this technique were so good that they captured the attention of a great number of researchers, leading to a proliferation of various flavors of GAN, each performing better than the others on a specific domain.} It's difficult anyway to understand how to compare different GAN models, because of the lack of a consistent metric and the different architectures with which networks can be designed, which for each project are related to the corresponding computational budget. A tentative to define some guidelines to avoid these problems, together with a fair and comprehensive comparison of state-of-the-art GANs, is discussed in \cite{46506}[MISSING ACCENTS ON REF]: what emerges here is that the computational budget plays a major role, allowing bad algorithms to outperform good ones if given enough time; plus, despite many claims of algorithms being superior to the original ones, there is no empirical evidence that they are across all datasets, in fact in most of them the original model outperforms those algorithms. [INSERT TABLE WITH LOSSES].
We thus report here a formalization of the original problem \cite{NIPS2014_5423}, modeling it with a game theory approach in a more precise way, as done in \cite{2017arXiv171200679O}.

Deep Convolutional GAN (DCGAN) works as follows: the generator G maps an input noise random variable $z\sim U([-1,1])$ to the data space as $G(z;\theta_g)$, where $\theta_g$ stands for the parameters of the network G. The discriminator network D defines a mapping between the data space and a scalar $D(x,\theta_d)$, where $x$ can be an element either belonging to the training sample or generated by G, while $\theta_d$ represents the parameters of the network as before. $D(x)$ is defined as the probability of $x$ being true data: the purpose of D is then to assign values close to one when $x\sim p_d(x)$ and close to zero when $x=G(z), z\sim p_z(z)$; G instead has the opposite task. We can infer this behaviors by setting appropriate loss functions:
\begin{align*}
L^{(D)} = -\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] - \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\\
L^{(G)} = - L^{(D)}
\end{align*}
where each player has to minimize its own loss.
This competition can be naturally represented as a 2-player zero-sum infinite game: the player entities, G and D, are two neural networks, each with a given architecture defining its depth and width; pure strategies are the different combinations of weights for the given architecture, and it is immediate to notice that these numbers are infinite; payoffs are defined as the opposite of their loss functions, which are also used for the training phase of the networks.
An infinite game doesn't guarantee the existence of a NE, not even in mixed strategies, nor it allows to find it as the value of the game, i.e. maximinimizing the payoffs. As pointed out in  \cite{2017arXiv171200679O} though, when using floating point numbers, the number of combinations of strategies becomes finite, albeit very large: there is then a NE, at least in mixed strategies, which however is hard to compute. It has to be kept into account also the fact that the optimization of a neural network can lead to a Local NE (LNE) because the problem is non-convex.
In strategic form, GAN games can be described by a tuple $(\{G, D\}, \{S_G, S_D\}, \{u_G, u_D\})$, where $S_G$ and $S_D$ are the (finite) sets of strategies, whit elements $\theta_g$ and $\theta_d$ respectively, while $u_G=-L_G$ and $u_D=-L_D$ are the corresponding payoffs, for G and D respectively.
The existence of a NE for this finite zero-sum game is granted can be found by maximinimizing its value:
\begin{align*}
\min\limits_{G} \max\limits_{D} \big\{V(D,G)\big\} =\\ \min\limits_{G} \max\limits_{D} \Big\{\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))] \Big\}.
\end{align*}
To solve this problem, we can first fix a generator $\bar{G}$ and compute the optimal discriminator $D$, which is a simple optimization problem:
\begin{align*}
	D^*(x) = arg\max\limits_{D} \big\{V(D,\bar{G}) \big\}
\end{align*}
where, in continuous form, we can write
\begin{align*}
	V(\bar{G},D) =\\
	 \int_x p_{data(x)} \log(D(x)) dx + \int_z p_{z}(z) \log(1-D(\bar{G}(z))) dz =\\ 
	= \int_x \Big[p_{data(x)} \log(D(x)) + p_{g}(x) \log(1-D(x)) \Big]dx =\\
	= \int_x L\big(x,D(x),\dot{D}(x)\big)dx.
\end{align*}
If $D(x)$ is an optimal point it satisfies the Euler-Lagrange equation
\begin{align*}
	\frac{\partial L}{\partial D} = \frac{d L}{d x} \frac{\partial L}{\partial \dot{D}}
\end{align*}
and since $\partial L / \partial \dot{D} = 0$ we get:
\begin{align*}
\frac{\partial L}{\partial D} = \frac{p_{data}(x)}{D(x)} - \frac{p_g(x)}{1-D(x)} = 0 \\ 
\Rightarrow \ \ D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}.
\end{align*}

The global minimum is found by setting $p_g = p_{data}$, in which case the value for the cost function becomes $-log(4)$.

It can be noticed that the payoffs are defined as the opposite of binary cross-entropy loss functions: this is particularly effective for discrimination tasks, but it's not for generation ones, because in the beginning of the training phase, when G is poor and D is able to correctly recognize generated samples, $log(1-D(G(z)))$ \textit{saturates} to zero thus resulting in a poor gradient. The learning in that case is too slow, but this problem can be avoided changing the loss function for G: instead of minimizing $log(1-D(G(z)))$, it can maximize $log(D(G(z)))$. This is formally defined as a Non-Saturating GAN (NSGAN), where the losses to be minimized are:
\begin{align*}
L^{(D)} = -\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] - \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))]\\
L^{(G)} = - log(D(G(z)))
\end{align*}

This new game isn't zero-sum any more, but the same equilibrium point of the dynamics can be found as before, thus providing the same theoretical results.
